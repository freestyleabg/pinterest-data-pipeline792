{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "\n",
    "\n",
    "class AWSManager:\n",
    "    def __init__(self):\n",
    "        self\n",
    "\n",
    "    def load_aws_credentials(self):\n",
    "        delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "        aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "        ACCESS_KEY = aws_keys_df.select(\"Access key ID\").collect()[0][\"Access key ID\"]\n",
    "        SECRET_KEY = aws_keys_df.select(\"Secret access key\").collect()[0][\n",
    "            \"Secret access key\"\n",
    "        ]\n",
    "        ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n",
    "        return ACCESS_KEY, SECRET_KEY, ENCODED_SECRET_KEY\n",
    "\n",
    "    def mount_s3_bucket(self):\n",
    "        ACCESS_KEY, SECRET_KEY, ENCODED_SECRET_KEY = self.load_aws_credentials()\n",
    "        AWS_S3_BUCKET = \"user-0a5040edb649-bucket\"\n",
    "        MOUNT_NAME = \"/mnt/user-0a5040edb649-bucket\"\n",
    "        SOURCE_URL = \"s3n://{0}:{1}@{2}\".format(\n",
    "            ACCESS_KEY, ENCODED_SECRET_KEY, AWS_S3_BUCKET\n",
    "        )\n",
    "        try:\n",
    "            dbutils.fs.mount(SOURCE_URL, MOUNT_NAME)\n",
    "            print(f\"Successfully mounted {SOURCE_URL} to {MOUNT_NAME}.\")\n",
    "        except Exception as e:\n",
    "            if \"already mounted\" in str(e):\n",
    "                print(\n",
    "                    f\"{MOUNT_NAME} already mounted. You can unmount using dbutils.fs.unmount('{MOUNT_NAME}') if needed.\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"Error mounting {SOURCE_URL} to {MOUNT_NAME}: {e}\")\n",
    "\n",
    "\n",
    "class DataTransformer:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def process_pindata(self):\n",
    "        # Replace missing values specific to df_pin\n",
    "        missing_values = {\n",
    "            \"follower_count\": \"User Info Error\",\n",
    "            \"description\": \"No description available%\",\n",
    "            \"tag_list\": \"N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e\",\n",
    "            \"image_src\": \"Image src error.\",\n",
    "            \"poster_name\": \"User Info Error\",\n",
    "            \"title\": \"No Title Data Available\",\n",
    "        }\n",
    "        for column, value in missing_values.items():\n",
    "            self.df = self.df.withColumn(\n",
    "                column, when(col(column).like(value), None).otherwise(col(column))\n",
    "            )\n",
    "        # Transformations specific to df_pin\n",
    "        self.df = self.df.withColumn(\n",
    "            \"follower_count\", regexp_replace(\"follower_count\", \"k\", \"000\")\n",
    "        )\n",
    "        self.df = self.df.withColumn(\n",
    "            \"follower_count\", regexp_replace(\"follower_count\", \"M\", \"000000\")\n",
    "        )\n",
    "        self.df = self.df.withColumn(\n",
    "            \"follower_count\", col(\"follower_count\").cast(\"int\")\n",
    "        )\n",
    "        self.df = self.df.withColumn(\n",
    "            \"save_location\", regexp_replace(\"save_location\", \"Local save in \", \"\")\n",
    "        )\n",
    "        self.df = self.df.withColumnRenamed(\"index\", \"ind\")\n",
    "        pin_column_order = [\n",
    "            \"ind\",\n",
    "            \"unique_id\",\n",
    "            \"title\",\n",
    "            \"description\",\n",
    "            \"follower_count\",\n",
    "            \"poster_name\",\n",
    "            \"tag_list\",\n",
    "            \"is_image_or_video\",\n",
    "            \"image_src\",\n",
    "            \"save_location\",\n",
    "            \"category\",\n",
    "        ]\n",
    "        self.df = self.df.select(pin_column_order)\n",
    "        return self.df\n",
    "\n",
    "    def process_geodata(self):\n",
    "        self.df = self.df.withColumn(\n",
    "            \"coordinates\", array(col(\"latitude\"), col(\"longitude\"))\n",
    "        )\n",
    "        self.df = self.df.drop(\"latitude\", \"longitude\")\n",
    "        self.df = self.df.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "        geo_column_order = [\"ind\", \"country\", \"coordinates\", \"timestamp\"]\n",
    "        self.df = self.df.select(geo_column_order)\n",
    "        return self.df\n",
    "\n",
    "    def process_userdata(self):\n",
    "        self.df = self.df.withColumn(\n",
    "            \"user_name\", concat_ws(\" \", \"first_name\", \"last_name\")\n",
    "        )\n",
    "        self.df = self.df.drop(\"first_name\", \"last_name\")\n",
    "        self.df = self.df.withColumn(\"date_joined\", to_timestamp(\"date_joined\"))\n",
    "        user_column_order = [\"ind\", \"user_name\", \"age\", \"date_joined\"]\n",
    "        self.df = self.df.select(user_column_order)\n",
    "        return self.df\n",
    "\n",
    "\n",
    "class BatchProcessor:\n",
    "    def __init__(self):\n",
    "        self.topics = [\"pin\", \"geo\", \"user\"]\n",
    "\n",
    "    def load_topics_into_dataframe(self, topic):\n",
    "        try:\n",
    "            file_path = f\"/mnt/user-0a5040edb649-bucket/topics/0a5040edb649.{topic}/partition=0/*.json\"\n",
    "            df = spark.read.format(\"json\").option(\"inferSchema\", \"true\").load(file_path)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load data for topic {topic}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def start_batch_pipeline(self):\n",
    "        dfs = {}\n",
    "        transformers = {}\n",
    "        for topic in self.topics:\n",
    "            try:\n",
    "                dfs[topic] = self.load_topics_into_dataframe(topic)\n",
    "                transformers[topic] = DataTransformer(dfs[topic])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing data for {topic}: {e}\")\n",
    "                break\n",
    "\n",
    "        return (\n",
    "            transformers[\"pin\"].process_pindata(),\n",
    "            transformers[\"geo\"].process_geodata(),\n",
    "            transformers[\"user\"].process_userdata(),\n",
    "        )\n",
    "\n",
    "\n",
    "class StreamProcessor:\n",
    "    def __init__(self, aws_manager):\n",
    "        self.aws_manager = aws_manager\n",
    "\n",
    "    # Define a function to read streaming data from a Kinesis stream\n",
    "    def get_stream_data(self, topic):\n",
    "        ACCESS_KEY, SECRET_KEY, ENCODED_SECRET_KEY = (\n",
    "            self.aws_manager.load_aws_credentials()\n",
    "        )\n",
    "        # Read the data stream from Kinesis using the specified parameters\n",
    "        stream_df = (\n",
    "            spark.readStream.format(\"kinesis\")\n",
    "            .option(\"streamName\", f\"streaming-0a5040edb649-{topic}\")\n",
    "            .option(\"initialPosition\", \"earliest\")\n",
    "            .option(\"region\", \"us-east-1\")\n",
    "            .option(\"awsAccessKey\", ACCESS_KEY)\n",
    "            .option(\"awsSecretKey\", SECRET_KEY)\n",
    "            .load()\n",
    "        )\n",
    "        # Return the streaming DataFrame\n",
    "        return stream_df\n",
    "\n",
    "    # Define a function to create a DataFrame from a streaming DataFrame\n",
    "    def create_df_from_stream(self, stream, schema):\n",
    "        # Select the 'data' column and cast it as a string\n",
    "        json_df = stream.selectExpr(\"CAST(data as STRING) as json_data\")\n",
    "        # Extract the structured JSON data using the specified schema and alias it as 'data'\n",
    "        structrued_df = json_df.select(\n",
    "            \"json_data\", from_json(json_df.json_data, schema).alias(\"data\")\n",
    "        )\n",
    "        # Select the fields from the structured 'data' column and return the DataFrame\n",
    "        return structrued_df.select(\"data.*\")\n",
    "\n",
    "    def create_topic_dataframe(self, topic):\n",
    "        schema_dict = {\"pin\": pin_schema, \"geo\": geo_schema, \"user\": user_schema}\n",
    "        topic_stream = self.get_stream_data(topic)\n",
    "        return self.create_df_from_stream(topic_stream, schema_dict[topic])\n",
    "\n",
    "    # Define a function to write a DataFrame to a Delta table as a streaming output\n",
    "    def write_df_to_table(self, df, table_name):\n",
    "        checkpoint_location = \"/tmp/kinesis/_checkpoints/\"\n",
    "        query = (\n",
    "            df.writeStream.format(\"delta\")\n",
    "            .outputMode(\"append\")\n",
    "            .option(\"checkpointLocation\", checkpoint_location)\n",
    "            .table(table_name)\n",
    "        )\n",
    "        return query\n",
    "\n",
    "    def start_stream_pipeline(self):\n",
    "        topics = [\"pin\", \"geo\", \"user\"]\n",
    "        dfs = {}\n",
    "        transformers = {}\n",
    "\n",
    "        for topic in topics:\n",
    "            try:\n",
    "                dfs[topic] = self.create_topic_dataframe(topic)\n",
    "                transformers[topic] = DataTransformer(dfs[topic])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing data for {topic}: {e}\")\n",
    "                break\n",
    "\n",
    "        return (\n",
    "            transformers[\"pin\"].process_pindata(),\n",
    "            transformers[\"geo\"].process_geodata(),\n",
    "            transformers[\"user\"].process_userdata(),\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
